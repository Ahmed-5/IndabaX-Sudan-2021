{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Text Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXA8fNtCceg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # matrix manuplation library\n",
        "np.random.seed(1000)\n",
        "import tensorflow as tf # tensorflow gpu or cpu based neural network library\n",
        "import string\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQxmDiuVduRL",
        "colab_type": "code",
        "outputId": "14a68206-c760-4ec7-cf29-99ec79a19681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/StarBoy01/IndabaX-Sudan-2019/master/nlp/TextClassification.zip\"\n",
        "!unzip \"TextClassification.zip\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-25 07:31:06--  https://raw.githubusercontent.com/StarBoy01/IndabaX-Sudan-2019/master/nlp/TextClassification.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24064147 (23M) [application/zip]\n",
            "Saving to: ‘TextClassification.zip.1’\n",
            "\n",
            "\rTextClassification.   0%[                    ]       0  --.-KB/s               \rTextClassification. 100%[===================>]  22.95M   131MB/s    in 0.2s    \n",
            "\n",
            "2019-10-25 07:31:06 (131 MB/s) - ‘TextClassification.zip.1’ saved [24064147/24064147]\n",
            "\n",
            "Archive:  TextClassification.zip\n",
            "replace bbc/business/001.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G3clOIzcehE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# these are the 5 classes in which the BBC news are organized in. \n",
        "# the folders in the datatsets are named after these classes\n",
        "folders = ['tech', 'business', 'entertainment', 'sport', 'politics'] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCAgDS4jcehK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a dictionary to collect all the data under the topic of the news\n",
        "# the key of the dictionary will be the type of the news\n",
        "# the value of the dictionary is a list of similar news under a topic\n",
        "data = {f:[] for f in folders}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGmeDi47cehP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vocab is a dictionary that contains the words in the BBC dataset.\n",
        "# To clean up the dataset, we only remove all the panctuation.\n",
        "# The loop will collect all news textunder the data dictionary from all folders\n",
        "vocab = {}\n",
        "exclude = set(string.punctuation)\n",
        "for folder in folders:\n",
        "    folder_path = '/content/bbc/' + folder\n",
        "    files = os.listdir(folder_path)\n",
        "    for file in files:\n",
        "        file_path = folder_path + \"/\" + file\n",
        "        text = open(file_path, encoding = 'unicode_escape').readlines()\n",
        "        text = ''.join(ch for ch in text if ch not in exclude)\n",
        "        data[folder].append(text)\n",
        "        for word in text.split(' '):\n",
        "            vocab[word] = word\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ek3EdNDoU1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['business'][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szJn3XrjcehV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec= {}\n",
        "wordvec_file = open('bbc_vec', encoding='utf-8')\n",
        "v, e = wordvec_file.readline()[:-1].split()\n",
        "vocab_size, embed_size = int(v), int(e)\n",
        "for i in range(vocab_size):\n",
        "    line = wordvec_file.readline()\n",
        "    line = line[:-1].split(' ')\n",
        "    word = line[0]\n",
        "    vec = [float(x) for x in line[1:]]\n",
        "    word2vec[word] = np.array(vec)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiRD3A9yceha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mean(words, word2vec):\n",
        "    \"\"\"\n",
        "    Given a set of words, it returns the mean of the vectors of these words' vectors\n",
        "    words: list of words\n",
        "    word2vec: a dictionary that contains key value pair of word and their vectors\n",
        "    \"\"\"\n",
        "    vecs = []\n",
        "    for word in words:\n",
        "        if word in word2vec:\n",
        "            vecs.append(word2vec[word])\n",
        "    if len(vecs) == 0:\n",
        "        return None\n",
        "    else:\n",
        "        vec = np.mean(np.vstack(vecs), axis=0)\n",
        "        return vec / np.linalg.norm(vec)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT3J_HeXcehe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X is a list that will contain vectors words. Y will contain their respective \n",
        "# labels these textes.\n",
        "X = []\n",
        "Y = []\n",
        "for key in data: # for each topic of the news\n",
        "    topic = key\n",
        "    news = data[key]\n",
        "    for new in news: # for each news in the selected topic\n",
        "        words = new.split(' ')\n",
        "        vec = get_mean(words, word2vec)\n",
        "        if vec is not None:\n",
        "            label = folders.index(topic)\n",
        "            yl = [0]*len(folders)\n",
        "            yl[label] = 1\n",
        "            Y.append(yl)\n",
        "            X.append(vec)\n",
        "X = np.vstack(X)\n",
        "Y = np.array(Y, dtype=np.int32) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwxwI_s_cehj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# randomize the dataset and split it into training and test set\n",
        "indexes = np.arange(len(X), dtype=np.int32)\n",
        "np.random.shuffle(indexes)\n",
        "X = X[indexes]\n",
        "Y = Y[indexes]\n",
        "n_train = int(.7 * X.shape[0])\n",
        "n_test = X.shape[0] - n_train\n",
        "print(\"Train Set: \", n_train, \"Test Set: \", n_test)\n",
        "x_train, y_train, x_test, y_test = X[:n_train], Y[:n_train], X[n_train:], Y[n_train:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW9E1x3scehp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a generator function that generates a pair of vector and its label when the generator is called\n",
        "def gen_data(X, Y, batch_size=32):\n",
        "    indexes = np.arange(len(X))\n",
        "    current = 0\n",
        "    while True:\n",
        "        bs = indexes[current:current + batch_size]\n",
        "        batch_x = X[bs]\n",
        "        batch_y = Y[bs]\n",
        "        current += batch_size\n",
        "        if current > len(X):\n",
        "            np.random.shuffle(indexes)\n",
        "            current = 0\n",
        "        yield batch_x, batch_y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6av3Xi5ceht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "steps_in_epoch = len(x_train) // batch_size\n",
        "gen = gen_data(x_train, y_train, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vhr1toCcehy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def base_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Dense(5, input_dim=embed_size, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
        "    return model\n",
        "model = base_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxLJZ3Egceh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(gen, steps_per_epoch=steps_in_epoch, epochs=100, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rRpCnWqceh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(text):\n",
        "    vec = get_mean(text.split(' '), word2vec)\n",
        "    vec = vec.reshape((1, embed_size))\n",
        "    probs = model.predict(vec)\n",
        "    label = probs.argmax()\n",
        "    return folders[label]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhsrmWlzceiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(\"this is a test news with atheletics\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87Z8DLVpceiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}